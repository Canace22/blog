---
title: 为什么AI大模型需要“显卡”/TPU、GPU与AMD之间有什么区别
description: 'GPU在训练大模型时效率远超CPU，TPU专为AI设计，AMD显卡性价比高但生态较弱。'
categories: 'AI探索'
tags: AI原理
author: Canace
comments: true
date: 2025-11-26 09:29:32
---
作为韭菜，每天都会看看财经博主的新闻，刷到刘备今天的日报里面提到谷歌TPU出来可能会重创英伟达，还提到了AMD，之前做过一段时间云游戏，开会也是总听团队小伙伴提到显卡之类的，发现这是自己的知识盲区，跟deepseek对话了几轮，做个总结。

## TPU/GPU/AMD/CUDA

### CPU (Central Processing Unit)

CPU 我们很熟悉，经常听到“大脑 CPU 在燃烧之类”的调侃，比较平民化的词，它是多才多艺的领导，善于按顺序处理多个复杂任务。

### GPU (Graphics Processing Unit)

GPU 在前端开发领域，特别是做图形方面的，会比较熟悉，我们在画图为了性能都会选择 GPU 渲染。看名称可以知道一开始专注做图形处理，现在延生到了更广阔的其他领域，比如训练 AI 模型。在大任务或者重复任务上能力卓越，会把大任务或重复任务拆分成更多小任务，并行处理。

### TPU (Tensor Processing Unit)

看缩写不知道是干嘛的，看了英文单词，看到 Tensor 觉得很熟悉，想到了 Tensorflow，就是前些年机器学习领域比较火的那个框架，TPU 就是为机器学习而生的卡，在处理线性代数计算方面技艺高超，主要通过谷歌云服务使用，不能直接购买和部署。

### AMD

一家公司，英伟达的竞争对手，卖 GPU 显卡，但是他的生态没有英伟达的好，英伟达的 CUDA 可以让开发者很容易用编程语言调用 GPU 进行并行计算，AMD 用的 ROCm，起步晚，兼容性、易用性、以及社区支持在很长一段时间里都不如CUDA，调试和使用难度较高，但是性价比较高。

## 为什么大模型离不开 GPU 们

打个比方，训练一个如GPT-4这样拥有万亿参数的大模型：

- CPU：按顺序执行，万亿个任务，可能需要数百年。
- GPU：并行处理任务，可能只需要几周或几个月。

结合上面的了解和这个例子，我们可以知道 GPU 的优势：

- 并行架构：数千核心能同时处理海量计算任务。
- 高内存带宽：GPU显存的带宽是CPU内存的10-20倍，能更快地“喂饱”饥渴的计算核心。
- 成熟的生态：经过多年发展，AI框架（如PyTorch, TensorFlow）已深度优化，能充分发挥GPU的性能。
